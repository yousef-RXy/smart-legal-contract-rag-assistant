{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:12.249634Z",
     "iopub.status.busy": "2026-02-20T08:53:12.247817Z",
     "iopub.status.idle": "2026-02-20T08:53:12.255263Z",
     "shell.execute_reply": "2026-02-20T08:53:12.254022Z",
     "shell.execute_reply.started": "2026-02-20T08:53:12.249584Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U -q \\\n",
    "    \"torchvision\" \\\n",
    "    \"torch\" \\\n",
    "    \"transformers>=4.40.0\" \\\n",
    "    \"sentence-transformers\" \\\n",
    "    \"accelerate>=0.29.0\" \\\n",
    "    \"bitsandbytes\" \\\n",
    "    \"langchain-huggingface\" \\\n",
    "    \"langchain-text-splitters\" \\\n",
    "    \"faiss-cpu\" \\\n",
    "    \"pypdf\" \\\n",
    "    \"gradio\" \\\n",
    "    \"torchaudio\" \\\n",
    "    \"peft\" \\\n",
    "    \"langchain\" \\\n",
    "    \"pymupdf\" \\\n",
    "    \"langchain-core\" \\\n",
    "    \"langchain-community\" \\\n",
    "    \"python-multipart\" \\\n",
    "    \"fastapi\" \\\n",
    "    \"uvicorn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:12.258009Z",
     "iopub.status.busy": "2026-02-20T08:53:12.257762Z",
     "iopub.status.idle": "2026-02-20T08:53:12.270315Z",
     "shell.execute_reply": "2026-02-20T08:53:12.268951Z",
     "shell.execute_reply.started": "2026-02-20T08:53:12.257986Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import torch\n",
    "import shutil\n",
    "import threading\n",
    "import uvicorn\n",
    "import requests\n",
    "import gradio as gr\n",
    "import time\n",
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextIteratorStreamer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from fastapi import FastAPI, UploadFile\n",
    "from fastapi.responses import StreamingResponse\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:12.272889Z",
     "iopub.status.busy": "2026-02-20T08:53:12.272176Z",
     "iopub.status.idle": "2026-02-20T08:53:12.278700Z",
     "shell.execute_reply": "2026-02-20T08:53:12.277431Z",
     "shell.execute_reply.started": "2026-02-20T08:53:12.272858Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_model_name = \"BAAI/bge-m3\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=4096,\n",
    "    do_sample=True,\n",
    "    temperature=0.4,\n",
    "    streamer=None,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "print(f\"Loaded {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:12.282233Z",
     "iopub.status.busy": "2026-02-20T08:53:12.281260Z",
     "iopub.status.idle": "2026-02-20T08:53:12.293496Z",
     "shell.execute_reply": "2026-02-20T08:53:12.292430Z",
     "shell.execute_reply.started": "2026-02-20T08:53:12.282200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"RAG Server\")\n",
    "\n",
    "vector_store = None\n",
    "ingested_files = set()\n",
    "\n",
    "def parse_pdf(file_info):\n",
    "    path, original_filename = file_info\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(path)\n",
    "        docs = loader.load()\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            loader = PyPDFLoader(path)\n",
    "            docs = loader.load()\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to parse {original_filename}: {e2}\")\n",
    "            return []\n",
    "            \n",
    "    for doc in docs:\n",
    "        doc.metadata['source'] = original_filename\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:12.296497Z",
     "iopub.status.busy": "2026-02-20T08:53:12.294893Z",
     "iopub.status.idle": "2026-02-20T08:53:12.312308Z",
     "shell.execute_reply": "2026-02-20T08:53:12.310930Z",
     "shell.execute_reply.started": "2026-02-20T08:53:12.296473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@app.post(\"/ingest\")\n",
    "async def ingest_files(files: list[UploadFile]):\n",
    "    global vector_store, ingested_files\n",
    "    \n",
    "    temp_dir = \"temp_server_files\"\n",
    "    if os.path.exists(temp_dir): shutil.rmtree(temp_dir)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    saved_files = [] \n",
    "    \n",
    "    try:\n",
    "        for file in files:\n",
    "            path = os.path.join(temp_dir, file.filename)\n",
    "            with open(path, \"wb\") as f:\n",
    "                shutil.copyfileobj(file.file, f)\n",
    "            saved_files.append((path, file.filename))\n",
    "            ingested_files.add(file.filename)\n",
    "            \n",
    "        all_docs = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            results = list(executor.map(parse_pdf, saved_files))\n",
    "            \n",
    "        for res in results:\n",
    "            all_docs.extend(res)\n",
    "\n",
    "        if not all_docs:\n",
    "            return {\"status\": \"error\", \"message\": \"No text extracted from files.\"}\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=400, \n",
    "            chunk_overlap=50,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        splits = text_splitter.split_documents(all_docs)\n",
    "        \n",
    "        if vector_store is None:\n",
    "            vector_store = FAISS.from_documents(splits, embeddings)\n",
    "        else:\n",
    "            vector_store.add_documents(splits)\n",
    "        \n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return {\"status\": \"success\", \"chunks\": len(splits), \"files\": len(saved_files)}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:12.356170Z",
     "iopub.status.busy": "2026-02-20T08:53:12.355588Z",
     "iopub.status.idle": "2026-02-20T08:53:12.377821Z",
     "shell.execute_reply": "2026-02-20T08:53:12.376688Z",
     "shell.execute_reply.started": "2026-02-20T08:53:12.356081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@app.post(\"/chat_stream\")\n",
    "async def chat_stream(data: dict):\n",
    "    question = data.get(\"message\", \"\")\n",
    "    if vector_store is None:\n",
    "        def err(): yield \"Please ingest documents first.\"\n",
    "        return StreamingResponse(err(), media_type=\"text/plain\")\n",
    "\n",
    "    def get_tokens(text):\n",
    "        return set(re.split(r'[^a-z0-9]+', text.lower()))\n",
    "\n",
    "    query_tokens = get_tokens(question)\n",
    "    query_nums = {t for t in query_tokens if t.isdigit()}\n",
    "    \n",
    "    STOP_WORD_STEMS = {\n",
    "        'contract', 'agreement', 'pdf', 'file', 'the', 'in', 'of', 'and', 'or', \n",
    "        'to', 'for', 'with', 'a', 'service', 'what', 'are', 'is', \n",
    "        'this', 'can', 'be', 'how', 'why', 'who', 'do', 'does', 'under', 'which',\n",
    "        'about', 'these', 'those', 'say', 'work'\n",
    "    }\n",
    "\n",
    "    def get_stem(word):\n",
    "        if word.endswith('ies') and len(word) > 4:\n",
    "            return word[:-3] + 'y'\n",
    "        if word.endswith('es') and len(word) > 4:\n",
    "            return word[:-2]\n",
    "        if word.endswith('s') and not word.endswith('ss') and len(word) > 3:\n",
    "            return word[:-1]\n",
    "        return word\n",
    "\n",
    "    search_all_intent = any(w in query_tokens for w in ['all'])\n",
    "\n",
    "    query_words = set()\n",
    "    for t in query_tokens:\n",
    "        if t in query_nums:\n",
    "            continue\n",
    "        \n",
    "        stemmed_t = get_stem(t)\n",
    "        \n",
    "        if stemmed_t not in STOP_WORD_STEMS and len(stemmed_t) > 3:\n",
    "            query_words.add(stemmed_t)\n",
    "            \n",
    "    target_files = []\n",
    "\n",
    "    if search_all_intent:\n",
    "        target_files = list(ingested_files)\n",
    "    else:\n",
    "        for file_name in ingested_files:\n",
    "            base_name = file_name.rsplit('.', 1)[0].lower()\n",
    "            file_tokens = {get_stem(t) for t in get_tokens(base_name)}\n",
    "            file_nums = {t for t in file_tokens if t.isdigit()}\n",
    "            \n",
    "            is_match = False\n",
    "            \n",
    "            if query_nums and file_nums:\n",
    "                if not (query_nums & file_nums):\n",
    "                    continue \n",
    "    \n",
    "            if query_nums & file_nums:\n",
    "                is_match = True\n",
    "            elif query_words & file_tokens:\n",
    "                is_match = True\n",
    "            elif not is_match and query_words:\n",
    "                clean_file_str = \"\".join(file_tokens) \n",
    "                for qw in query_words:\n",
    "                    if len(qw) > 4 and qw in clean_file_str:\n",
    "                        is_match = True\n",
    "                        break\n",
    "    \n",
    "            if is_match:\n",
    "                target_files.append(file_name)\n",
    "\n",
    "    target_files = list(set(target_files))\n",
    "    \n",
    "    print(f\"Query: '{question}'\\n   -> Extracted Nums: {query_nums}\\n   -> Extracted Words: {query_words}\\n   -> MATCHED FILES: {target_files if target_files else 'ALL FILES'}\")\n",
    "\n",
    "    final_docs = []\n",
    "\n",
    "    if target_files:\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\", \n",
    "            search_kwargs={\n",
    "                'k': 10, \n",
    "                'filter': lambda metadata: metadata.get(\"source\") in target_files\n",
    "            }\n",
    "        )\n",
    "        final_docs = retriever.invoke(question)\n",
    "\n",
    "        found_sources = set(d.metadata['source'] for d in final_docs)\n",
    "        for target in target_files:\n",
    "            if target not in found_sources:\n",
    "                from langchain_core.documents import Document\n",
    "                final_docs.append(Document(\n",
    "                    page_content=f\"SYSTEM NOTE: User explicitly asked about '{target}', but no relevant text was found.\",\n",
    "                    metadata={'source': target}\n",
    "                ))\n",
    "    else:\n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\", \n",
    "            search_kwargs={\n",
    "                'k': 40,\n",
    "                'score_threshold': 0.25\n",
    "            }\n",
    "        )\n",
    "        clean_query_words = [w for w in question.lower().split() if get_stem(w) not in STOP_WORD_STEMS]\n",
    "        search_query = \" \".join(clean_query_words) if clean_query_words else question\n",
    "    \n",
    "        final_docs = retriever.invoke(search_query)\n",
    "\n",
    "        if not final_docs:\n",
    "            def empty_err(): yield \"No highly relevant sections found in the documents.\"\n",
    "            return StreamingResponse(empty_err(), media_type=\"text/plain\")\n",
    "\n",
    "    unique_sources = list(set([d.metadata['source'] for d in final_docs]))\n",
    "    num_docs = len(unique_sources)\n",
    "    \n",
    "    if num_docs > 3:\n",
    "        format_instr = \"## FORMATTING: You found data from >3 files. You **MUST** use a **Markdown Table**.\"\n",
    "    elif num_docs > 1:\n",
    "        format_instr = \"## FORMATTING: You found data from 2-3 files. Use a **Bulleted List**.\"\n",
    "    else:\n",
    "        format_instr = \"## FORMATTING: You found data from 1 file. Provide a direct, concise sentence. Do NOT use a table.\"\n",
    "\n",
    "    grouped_docs = {}\n",
    "    for d in final_docs:\n",
    "        source = d.metadata['source']\n",
    "        if source not in grouped_docs:\n",
    "            grouped_docs[source] = []\n",
    "        grouped_docs[source].append(d.page_content)\n",
    "\n",
    "    context_parts = []\n",
    "    for source, texts in grouped_docs.items():\n",
    "        combined_text = \"\\n...\".join(texts)\n",
    "        context_parts.append(f'<document filename=\"{source}\">\\n{combined_text}\\n</document>')\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You are an ultra-strict, literal-minded Legal Contract Auditor. Your ONLY job is to extract exact text from the provided context.\n",
    "\n",
    "{format_instr}\n",
    "\n",
    "### CRITICAL RULES (FAILURE IS NOT AN OPTION):\n",
    "1. **ABSOLUTE ISOLATION:** The context is divided by <document> tags. You MUST evaluate each document in a vacuum. A document cannot \"borrow\" or \"inherit\" numbers, names, or terms from another document.\n",
    "2. **ZERO HALLUCINATION:** If a specific document does not contain the answer, you are FORBIDDEN from looking at other documents to fill in the blank. \n",
    "3. **MANDATORY QUOTING:** For every single document, you must find the EXACT sentence that contains the answer. If you cannot find a relevant quote within that specific <document> block, the answer DOES NOT EXIST in that document.\n",
    "4. **HANDLING MISSING DATA:** If a document lacks the requested information, you MUST output exactly \"None found\" in the Exact Quote column and \"Not specified in document.\" in the Summary column. \n",
    "5. **TABLE FORMAT:** Use exactly three columns: | Document | Exact Quote | Summary |. Provide exactly ONE row per document. DO NOT put spaces before the table.\n",
    "7. **NO LOOPHOLES OR BREACH ASSISTANCE:** You are strictly FORBIDDEN from identifying legal loopholes, vulnerabilities, or methods to circumvent the contract. You must NEVER provide instructions, suggestions, or advice on how to breach the agreement, avoid obligations, or engage in illegal activities.\n",
    "\n",
    "### EXAMPLE OF CORRECT BEHAVIOR:\n",
    "If asked for \"hourly rate\", and Doc_A says \"$50/hr\" but Doc_B mentions no money:\n",
    "| Doc_A | \"The rate is $50/hr.\" | The rate is $50/hr. |\n",
    "| Doc_B | None found | Not specified in document. |\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    gen_kwargs = dict(inputs, streamer=streamer, max_new_tokens=1024, temperature=0.3, do_sample=True)\n",
    "    \n",
    "    thread = threading.Thread(target=model.generate, kwargs=gen_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    def generate():\n",
    "        sources_str = \", \".join(unique_sources)\n",
    "        if not sources_str: sources_str = \"None\"\n",
    "            \n",
    "        yield f\"**Context Searched:** {sources_str} |---| \\n\"\n",
    "        for new_text in streamer:\n",
    "            yield new_text\n",
    "\n",
    "    return StreamingResponse(generate(), media_type=\"text/plain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:12.380086Z",
     "iopub.status.busy": "2026-02-20T08:53:12.379697Z",
     "iopub.status.idle": "2026-02-20T08:53:12.396404Z",
     "shell.execute_reply": "2026-02-20T08:53:12.395257Z",
     "shell.execute_reply.started": "2026-02-20T08:53:12.380062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@app.post(\"/evaluate\")\n",
    "async def evaluate_system(data: dict):\n",
    "    num_questions = data.get(\"num_questions\", 3)\n",
    "    \n",
    "    if vector_store is None:\n",
    "        return {\"status\": \"error\", \"message\": \"No documents ingested. Please ingest files first.\"}\n",
    "        \n",
    "    docs = list(vector_store.docstore._dict.values())\n",
    "    if len(docs) < 2:\n",
    "        return {\"status\": \"error\", \"message\": \"Not enough documents. Need at least 2 chunks.\"}\n",
    "        \n",
    "    results = []\n",
    "    scores = []\n",
    "    \n",
    "    def generate_eval_text(prompt_text, max_tokens=512):\n",
    "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_tokens, temperature=0.1, do_sample=False)\n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "    \n",
    "    for i in range(num_questions):\n",
    "        doc = random.choice(docs)\n",
    "        source_file = doc.metadata.get(\"source\", \"the document\")\n",
    "        \n",
    "        context = f\"Document File Name: {source_file}\\n\\nContext:\\n{doc.page_content}\"\n",
    "        \n",
    "        qa_prompt = (\n",
    "            \"<|im_start|>system\\nUse the document provided to generate a HIGHLY SPECIFIC question-answer pair.\\n\"\n",
    "            \"The question must be narrow and point to a specific factual detail (e.g., a specific dollar amount, a specific timeframe, or a unique condition).\\n\"\n",
    "            \"CRITICAL: Do NOT ask broad summary questions like 'What are the key aspects?' or 'What does this document cover?'\\n\"\n",
    "            \"CRITICAL: You MUST explicitly include the 'Document File Name' in your generated question. (e.g., 'According to Contract_X.pdf, what is the...').\\n\"\n",
    "            \"Use the format:\\nQuestion: (your question)\\nAnswer: (your answer)\\n\"\n",
    "            \"DO NOT SAY ANYTHING ELSE.<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\n{context}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        qa_pair = generate_eval_text(qa_prompt)\n",
    "        \n",
    "        try:\n",
    "            synth_q = qa_pair.split(\"Answer:\")[0].replace(\"Question:\", \"\").strip()\n",
    "            synth_a = qa_pair.split(\"Answer:\")[1].strip()\n",
    "        except IndexError:\n",
    "            continue\n",
    "            \n",
    "        retrieved_docs = vector_store.as_retriever(search_kwargs={'k': 5}).invoke(synth_q)\n",
    "        rag_context = \"\\n\\n\".join([d.page_content for d in retrieved_docs])\n",
    "        rag_prompt = (\n",
    "            \"<|im_start|>system\\nAnswer the question using only the context below.\\n<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\nContext:\\n{rag_context}\\n\\nQuestion: {synth_q}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        rag_a = generate_eval_text(rag_prompt)\n",
    "        \n",
    "        eval_prompt = (\n",
    "            \"<|im_start|>system\\nEvaluate the following Question-Answer pair for accuracy and completeness.\\n\"\n",
    "            \"Assume 'Answer 1' is the Ground Truth.\\n\"\n",
    "            \"Criteria:\\n\"\n",
    "            \"[1] Answer 2 lies, contradicts the ground truth, fails to answer the question, or misses critical context.\\n\"\n",
    "            \"[2] Answer 2 is factually accurate, contains the same core information as the ground truth, and is equally good or better.\\n\\n\"\n",
    "            \"Output Format MUST start with the exact tag [1] or [2], followed by the justification.\\n\"\n",
    "            \"Example: [2] Answer 2 correctly identifies the same details as Answer 1.<|im_end|>\\n\"\n",
    "            f\"<|im_start|>user\\nQuestion: {synth_q}\\n\\nAnswer 1 (Ground Truth): {synth_a}\\n\\nAnswer 2 (New Answer): {rag_a}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        )\n",
    "        eval_res = generate_eval_text(eval_prompt)\n",
    "        \n",
    "        prefix = eval_res[:15]\n",
    "        score_val = 2 if \"[2]\" in prefix or \"Score: 2\" in prefix else 1\n",
    "        scores.append(score_val)\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": synth_q,\n",
    "            \"ground_truth\": synth_a,\n",
    "            \"rag_answer\": rag_a,\n",
    "            \"evaluation\": eval_res,\n",
    "            \"score\": score_val\n",
    "        })\n",
    "        \n",
    "    if not scores:\n",
    "        return {\"status\": \"error\", \"message\": \"Evaluation failed to generate properly formatted Q&A pairs.\"}\n",
    "        \n",
    "    pref_score = sum([1 for s in scores if s == 2]) / len(scores)\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"preference_score\": pref_score,\n",
    "        \"details\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:23.581471Z",
     "iopub.status.busy": "2026-02-20T08:53:23.581134Z",
     "iopub.status.idle": "2026-02-20T08:53:23.594605Z",
     "shell.execute_reply": "2026-02-20T08:53:23.590429Z",
     "shell.execute_reply.started": "2026-02-20T08:53:23.581443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running on http://localhost:7874\n"
     ]
    }
   ],
   "source": [
    "def start_server(port):\n",
    "    try:\n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=port, log_level=\"error\")\n",
    "        server = uvicorn.Server(config)\n",
    "        server.run()\n",
    "    except Exception as e:\n",
    "        print(f\"Port {port} failed: {e}\")\n",
    "\n",
    "if 'current_port' not in globals():\n",
    "    current_port = 7865\n",
    "else:\n",
    "    current_port += 1 \n",
    "\n",
    "if 'server_thread' in globals() and server_thread.is_alive():\n",
    "    print(\"Stopping existing server...\")\n",
    "    time.sleep(1)\n",
    "\n",
    "server_thread = threading.Thread(target=start_server, args=(current_port,), daemon=True)\n",
    "server_thread.start()\n",
    "print(f\"Server running on http://localhost:{current_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T08:53:27.003043Z",
     "iopub.status.busy": "2026-02-20T08:53:27.002745Z",
     "iopub.status.idle": "2026-02-20T08:53:28.757774Z",
     "shell.execute_reply": "2026-02-20T08:53:28.756211Z",
     "shell.execute_reply.started": "2026-02-20T08:53:27.003017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/3661341250.py:57: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7875\n",
      "* Running on public URL: https://47dcda7ffbcfa26100.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://47dcda7ffbcfa26100.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_files_client(files):\n",
    "    if not files: return \"No files selected.\"\n",
    "    file_payload = [('files', (os.path.basename(f), open(f, 'rb'), 'application/pdf')) for f in files]\n",
    "    try:\n",
    "        response = requests.post(f\"http://localhost:{current_port}/ingest\", files=file_payload)\n",
    "        res_data = response.json()\n",
    "        if res_data.get(\"status\") == \"success\":\n",
    "            return f\"Indexed {res_data.get('chunks')} chunks.\"\n",
    "        else:\n",
    "            return f\"Server Error: {res_data.get('message')}\"\n",
    "    except Exception as e:\n",
    "        return f\"Connection Failed: {str(e)}\"\n",
    "\n",
    "def chat_client(message, history):\n",
    "    try:\n",
    "        response = requests.post(f\"http://localhost:{current_port}/chat_stream\", json={\"message\": message}, stream=True)\n",
    "        full_text, sources, started = \"\", \"\", False\n",
    "        for chunk in response.iter_content(chunk_size=None, decode_unicode=True):\n",
    "            if not chunk: continue\n",
    "            if \"|---|\" in chunk and not started:\n",
    "                parts = chunk.split(\"|---|\")\n",
    "                sources = parts[0].replace(\"SOURCES:\", \"**Sources:**\")\n",
    "                full_text += parts[1]\n",
    "                started = True\n",
    "            else:\n",
    "                full_text += chunk\n",
    "            yield f\"{full_text}\\n\\n---\\n{sources}\"\n",
    "    except Exception as e:\n",
    "        yield f\"Stream Error: {str(e)}\"\n",
    "        \n",
    "def run_eval_client(num_qs):\n",
    "    try:\n",
    "        response = requests.post(f\"http://localhost:{current_port}/evaluate\", json={\"num_questions\": int(num_qs)})\n",
    "        data = response.json()\n",
    "        \n",
    "        if data.get(\"status\") == \"error\":\n",
    "            return data.get(\"message\"), \"\"\n",
    "            \n",
    "        score = data.get(\"preference_score\", 0)\n",
    "        \n",
    "        stats_md = f\"### Overall Preference Score: {score * 100:.2f}%\\n\"\n",
    "        stats_md += f\"*(The pipeline successfully answered and matched/exceeded the ground truth on {int(score * num_qs)} out of {num_qs} questions)*\"\n",
    "        \n",
    "        details_md = \"\"\n",
    "        for i, res in enumerate(data.get(\"details\", [])):\n",
    "            details_md += f\"### QA Pair {i+1}\\n\"\n",
    "            details_md += f\"**Question:** {res['question']}\\n\\n\"\n",
    "            details_md += f\"**Ground Truth (Synthetic):** {res['ground_truth']}\\n\\n\"\n",
    "            details_md += f\"**RAG Answer:** {res['rag_answer']}\\n\\n\"\n",
    "            details_md += f\"**Judge Evaluation:** {res['evaluation']}\\n\\n\"\n",
    "            details_md += \"---\\n\"\n",
    "            \n",
    "        return stats_md, details_md\n",
    "    except Exception as e:\n",
    "        return f\"**Error:** {str(e)}\", \"\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# Smart Legal Contract RAG Assistant\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Chat\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    files = gr.File(label=\"Upload PDFs\", file_count=\"multiple\")\n",
    "                    btn = gr.Button(\"Process\", variant=\"primary\")\n",
    "                    stat = gr.Textbox(label=\"Status\")\n",
    "                with gr.Column(scale=2):\n",
    "                    gr.ChatInterface(fn=chat_client)\n",
    "                    \n",
    "            btn.click(process_files_client, inputs=[files], outputs=[stat])\n",
    "\n",
    "        with gr.Tab(\"Evaluation\"):\n",
    "            gr.Markdown(\"### RAG Evaluation via LLM-as-a-Judge\\n\"\n",
    "                        \"This tool generates synthetic question-answer pairs from your ingested documents, \"\n",
    "                        \"runs the questions through your retrieval pipeline, and uses the LLM to score the RAG answer.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                num_qs_slider = gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of Test Questions\")\n",
    "                eval_btn = gr.Button(\"Run Evaluation Test\", variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    eval_stats = gr.Markdown(label=\"Statistics\")\n",
    "                    eval_details = gr.Markdown(label=\"Detailed Breakdown\")\n",
    "            \n",
    "            eval_btn.click(run_eval_client, inputs=[num_qs_slider], outputs=[eval_stats, eval_details])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
